{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22636940",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'german' # select your file's language\n",
    "''' ['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', \n",
    "'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', \n",
    "'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', \n",
    "'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', \n",
    "'spanish', 'swedish', 'tajik', 'turkish']\n",
    "'''\n",
    "tags = [lang, 'sentence::mining', 'Tatort', 'sentence::recognition'] # input the desired tags for ANKI\n",
    "fields = [lang, 'defs', tags] # decide on the number of fields and their contents for ANKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568ca828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the tar zip\n",
    "import tarfile\n",
    "\n",
    "tar_path = r\"Documents\\GitHub\\SRT2CSV\\de_core_news_sm-3.0.0.tar.gz\"\n",
    "extract_path = r\"Documents\\GitHub\\SRT2CSV\\de_core_news_sm-3.0.0\"\n",
    "\n",
    "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4cee9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das DET nk\n",
      "Auto NOUN sb\n",
      "ist AUX ROOT\n",
      "schnell ADV pd\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load from the correct folder\n",
    "nlp = spacy.load(r\"C:\\Users\\paro\\Documents\\GitHub\\SRT2CSV\\de_core_news_sm-3.0.0\\de_core_news_sm-3.0.0\\de_core_news_sm\\de_core_news_sm-3.0.0\")\n",
    "\n",
    "# Test it with a German sentence\n",
    "doc = nlp(\"Das Auto ist schnell.\")\n",
    "\n",
    "# Iterate through tokens\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26dac7f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (1596159479.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 25\u001b[1;36m\u001b[0m\n\u001b[1;33m    nlp = spacy.load(\"C:\\Users\\paro\\Documents\\GitHub\\SRT2CSV\\de_core_news_sm-3.0.0\\de_core_news_sm-3.0.0\\de_core_news_sm\\de_core_news_sm-3.0.0\") # loads german model\u001b[0m\n\u001b[1;37m                                                                                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "SRT2CSV\n",
    "vsulli\n",
    "26 April 2025\n",
    "read in a .srt file \n",
    "convert to a df to perform nlp\n",
    "export to a .csv file for upload to ANKI\n",
    "'''\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysrt\n",
    "import re\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "lang_sw = stopwords.words(lang)\n",
    "from textblob import TextBlob\n",
    "\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "nlp = spacy.load(\"C:\\Users\\paro\\Documents\\GitHub\\SRT2CSV\\de_core_news_sm-3.0.0\\de_core_news_sm-3.0.0\\de_core_news_sm\\de_core_news_sm-3.0.0\") # loads german model\n",
    "\n",
    "# module that allows you to get meanings, translations, synonyms, and antonyms for supported langs\n",
    "'''[bengali' (bn),'chinese (zh)', english' (en), 'french' (fr), 'german' (de), \n",
    "'italian' (it),'portuguese' (pt), 'romanian' (ro), 'russian'(ru), 'spanish'(es), \n",
    "'turkish' (tr)]'''\n",
    "from PyMultiDictionary import MultiDictionary, DICT_EDUCALINGO\n",
    "dictionary = MultiDictionary()\n",
    "\n",
    "# allows for displaying multiple outputs in one cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "063ce73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "# keeps full GUI from appearing\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "# shows dialog box to ask for filename\n",
    "filename = filedialog.askopenfilename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f26842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the encoding if UnicodeDecodeError\n",
    "# you can open the file in NotePad and check SaveAs for default encoding\n",
    "# ANSI, UTF-8\n",
    "subs = pysrt.open(filename, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_formatting(subfile):\n",
    "    # read through every sub line\n",
    "    # using regex, delete section of line between <>\n",
    "    # combine to string\n",
    "    # update index\n",
    "    for i in range(len(subfile)):\n",
    "        # replace all characters between < and > with \"\"\n",
    "        # using regex\n",
    "        # strip white space from beginning\n",
    "        subfile[i].text = subfile[i].text.lstrip()\n",
    "        subfile[i].text = re.sub(r'<c.vtt_\\w+>\\s*\"*', \"\", subfile[i].text)\n",
    "        subfile[i].text = re.sub(\"</c>\", \"\", subfile[i].text)\n",
    "        # replace \"-\" with \"\"\n",
    "        subfile[i].text = re.sub(\"-\", \"\", subfile[i].text)\n",
    "\n",
    "    return subfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = remove_formatting(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with desired column name\n",
    "def create_dataframe(subfile, col_name):\n",
    "    # create dataframe\n",
    "    df = pd.DataFrame(columns=[col_name])\n",
    "    for i in range(len(subfile)):\n",
    "        df.loc[i] = subfile[i].text\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee757108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine rows until period or end character reached\n",
    "# used for gaining more context for sentiment analysis and classification\n",
    "# append these rows to list and then create new df - more efficient than appending to df\n",
    "# end characters at index -1 must be ) . ? ! \n",
    "df = create_dataframe(subs, 'Subtitle')\n",
    "df\n",
    "new_df_list = []\n",
    "current_row = \"\"\n",
    "for row in df['Subtitle']:\n",
    "    if row[-1] == \")\" or row[-1] == \".\" or row[-1] == \"?\" or row[-1] == \"!\":\n",
    "        current_row += row\n",
    "        new_df_list.append(current_row)\n",
    "        current_row = \"\"\n",
    "    else:\n",
    "        current_row += row\n",
    "\n",
    "# create new df from the list of combined rows\n",
    "new_df = pd.DataFrame(new_df_list)\n",
    "new_df.rename(columns={0:\"Subtitle\"}, inplace=True)\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86256445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe removing the subtitle font tags\n",
    "# go row by row - only include what's between > and < symbols\n",
    "new_df_list = []\n",
    "current_row = \"\"\n",
    "for i in range(len(df['Subtitle'])):\n",
    "    # slice between two characters > and < \n",
    "    match = re.findall(r'>(.*?)<', df['Subtitle'][i])\n",
    "    if match:\n",
    "        new_df_list.append(match[0])\n",
    "    else:\n",
    "        new_df_list.append(df['Subtitle'][i])\n",
    "\n",
    "    \n",
    "# create new df from the list of combined rows\n",
    "# currently fewer rows - need to verify that it combined the rows correctly\n",
    "new_df = pd.DataFrame(new_df_list)\n",
    "new_df.rename(columns={0:\"Subtitle\"}, inplace=True)\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b172c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic exploratory data analysis\n",
    "def explore_data(dataframe, column):\n",
    "    print(dataframe.head())\n",
    "    print(dataframe.shape)\n",
    "    print(dataframe.dtypes)\n",
    "    print(dataframe.describe(include='all'))\n",
    "    \n",
    "explore_data(new_df, 'Subtitle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0da6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "def make_lowercase(df):\n",
    "    df['Lowercase'] = df['Subtitle'].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daaa78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "def remove_punctuation(df):\n",
    "    # ^ is a negation inside brackets (anything except)\n",
    "    # starts with any word, digits, or underscore, white space character, apostrophe, + means any character in the string\n",
    "    df['Lowercase'] = df['Lowercase'].str.replace(r\"[^\\w\\s']+\", ' ', regex = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove newline \\n character\n",
    "def remove_newline(df):\n",
    "    df['Subtitle'] = df['Subtitle'].str.replace(r'\\n', ' ', regex = True) # replaces the \\n with a space\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c554f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove newline from df\n",
    "clean_df = remove_newline(new_df)\n",
    "print(clean_df.head())\n",
    "\n",
    "clean_df[102:103]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lowercase\n",
    "clean_df = make_lowercase(clean_df)\n",
    "print(clean_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed93d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "clean_df = remove_punctuation(clean_df)\n",
    "print(clean_df.head())\n",
    "clean_df[102:103]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d47728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenizer\n",
    "# basic tokenizer splits on apostrophe\n",
    "# tweet tokenizer does not\n",
    "clean_df['Word Tokens'] = clean_df['Lowercase'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['Word Tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27209462",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['Sentence Tokens'] = clean_df['Lowercase'].apply(nltk.sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['Sentence Tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d4351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "\n",
    "# open existing file\n",
    "try:\n",
    "    with open(\"removed_words.json\", \"r\") as f:\n",
    "        removed_words = set(json.load(f))\n",
    "#JSONDecodeError - if file is empty       \n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    removed_words = set()\n",
    "    \n",
    "# add words \n",
    "removed_words.update(lang_sw)\n",
    "\n",
    "# save new set\n",
    "with open(\"removed_words.json\", \"w\") as f:\n",
    "    json.dump(list(removed_words), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4cad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend the stop words with custom words\n",
    "\n",
    "# names of characters & places \n",
    "custom_stop_words = ['hamburg', 'berlin', 'leipzig', 'elise', 'christian', 'berti', 'eva', 'chris', 'landsberger', 'trimmel']\n",
    "\n",
    "# save to JSON file\n",
    "try:\n",
    "    with open(\"removed_words.json\", \"r\") as f:\n",
    "        removed_words = set(json.load(f))\n",
    "except FileNotFoundError:\n",
    "    removed_words = set()\n",
    "    \n",
    "removed_words.update(custom_stop_words)\n",
    "\n",
    "with open(\"removed_words.json\", \"w\") as f:\n",
    "    json.dump(list(removed_words), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from stop words file\n",
    "desired_words = []\n",
    "\n",
    "try:\n",
    "    with open(\"removed_words.json\", \"r\") as f:\n",
    "        removed_words = set(json.load(f))\n",
    "  \n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    removed_words = set()\n",
    "    \n",
    "# add back words (remove from set)\n",
    "removed_words.difference_update(desired_words)\n",
    "\n",
    "with open(\"removed_words.json\", \"w\") as f:\n",
    "    json.dump(list(removed_words), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f5b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column without all removed words\n",
    "clean_df['No Stop Words'] = clean_df['Word Tokens'].apply(lambda x: [item for item in x if item not in removed_words])\n",
    "clean_df[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[100:101]['Subtitle']\n",
    "clean_df[0:20]['No Stop Words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a frequency diagram without stop words\n",
    "\n",
    "# list of all words\n",
    "words = []\n",
    "for index, row in clean_df.iterrows():\n",
    "    for word in row['No Stop Words']:\n",
    "        words.append(word)\n",
    "words[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9404383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the word frequency\n",
    "sns.set_style('darkgrid')\n",
    "freq_words = nltk.FreqDist(words)\n",
    "freq_words.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30555289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyMultiDictionary to get definition\n",
    "\n",
    "# retrieves part of speech, explanations\n",
    "print(dictionary.meaning('de', 'hund', dictionary=DICT_EDUCALINGO))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08dce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only desired language from definition\n",
    "def extract_desired_lang(word, lang):\n",
    "    # Get the dictionary result\n",
    "    res = dictionary.meaning(lang, word, dictionary=DICT_EDUCALINGO)\n",
    "    \n",
    "    if not res or not res[1]:\n",
    "        return None\n",
    "    \n",
    "    pos_tags, text, _ = res\n",
    "    \n",
    "    # split into sentences\n",
    "    sentences = text.split('. ')\n",
    "    \n",
    "    # desired language sentences\n",
    "    my_sentences = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        sent = sent.strip() \n",
    "        if not sent:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            detected_lang = detect(sent)\n",
    "            \n",
    "            # check if desired lang\n",
    "            if detected_lang == lang:\n",
    "                my_sentences.append(sent)\n",
    "        except LangDetectException:\n",
    "            # skip if not desired lang\n",
    "            continue\n",
    "    \n",
    "    # combine sentences and return\n",
    "    return ' '.join(my_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539aa3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop = ['katze', 'vogel']\n",
    "\n",
    "# construct a dictionary entry for all words in no stop words column\n",
    "# word [pos]: definition\n",
    "# space\n",
    "# word2 [pos]: definition\n",
    "dict_entry = \"\"\n",
    "for w in no_stop:\n",
    "    doc = nlp(w)\n",
    "    \n",
    "    if doc.pos == \"NOUN\":\n",
    "        article = doc.article_\n",
    "    else:\n",
    "        article = \"\"\n",
    "        \n",
    "    dict_entry += article + \" \" + w + \" [\" + dictionary.meaning('de', w, dictionary=DICT_EDUCALINGO)[0][0] + \"] : \" +  extract_desired_lang(w, 'de') + \"\\n\\n\"\n",
    "    \n",
    "print(dict_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export sentences to a .csv file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b12769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# change lang list to include abbreviations ['german', 'de']\n",
    "# rename json file by language DEU_removed_words, ESP_removed_words\n",
    "\n",
    "# add export of all sentences to a .csv file\n",
    "# fields \n",
    "    # DEU     # dict def of no-stop-words column    # tags\n",
    "    \n",
    "# change export to .csv to only include desired number of no stop words/unknown words (1, 2, 3) etc.\n",
    "\n",
    "# change educalingo dictionary to web scraping an actual dictionary for better definitions and articles for nouns\n",
    "    \n",
    "#  Parts of speech tagging - NLP\n",
    "\n",
    "# Notepad++ convert ANSI to UTF-8 for special characters\n",
    "# seems to need to be UTF-8 BOM in order to preserve special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b234dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install C:/Users\\paro/Documents/GitHub/SRT2CSV/de_core_news_sm-3.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e885f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the German model\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Test it on a sentence\n",
    "doc = nlp(\"Der Hund läuft im Park.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1da729",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip check\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acbfa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install click==7.1.2\n",
    "!pip install torch==2.3.0\n",
    "!pip install urllib3==1.26.6\n",
    "!pip install clyent==1.2.1 nbformat==5.4.0 python-dateutil==2.8.2 PyYAML==6.0 requests==2.28.1\n",
    "!pip install markdown-it-py==2.2.0\n",
    "!pip install fsspec==2023.3.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a685d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331878a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n spacy_env python=3.11\n",
    "conda activate spacy_env\n",
    "pip install spacy\n",
    "python -m spacy download de_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47229a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate spacy_env  # or whatever you named your environment\n",
    "pip install ipykernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755fb43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
